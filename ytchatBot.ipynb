{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "#stemmer = LancasterStemmer()\n",
    "#nltk.download('wordnet')  WordNet is the lexical database i.e. dictionary for the English language, specifically \n",
    "#designed for natural language processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"diseases.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [lemmatizer.lemmatize(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms=[\"hi\",\"hello\",\"bye\",\"thanks\",\"whats up\",\"Hey\"]\n",
    "with open(\"diseases1.csv\") as file1:\n",
    "    symp = csv.reader(file1, delimiter=',')\n",
    "    for row in symp:\n",
    "        for s in row:\n",
    "            symptoms.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\layers\\core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensorflow.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "\n",
    "#We are building a 3-layers neural network using TFLearn. We need to specify the shape of our input data. In our case,\n",
    "#each sample has a total of 318 features and we will process samples per batch to save memory, so our data \n",
    "#input shape is [None, 318] ('None' stands for an unknown dimension, so we can change the total number of samples \n",
    "#that are processed in a batch).\n",
    "net = tflearn.input_data(shape=[None, len(training[0])]) #to know length of features we use len(training[0])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") #Softmax function is used in the output layer of a \n",
    "                                    #neural net to represent a probability distribution of possible outcomes of the network.\n",
    "net = tflearn.regression(net) #used in TFLearn to apply a regression (linear or logistic) to the provided input. It requires\n",
    "                              #to specify a TensorFlow gradient descent optimizer 'optimizer' that will minimize the provided \n",
    "                              #loss function 'loss' '''\n",
    "\n",
    "model = tflearn.DNN(net) #TFLearn provides a model wrapper 'DNN' that can automatically performs a neural network\n",
    "                         #classifier tasks, such as training, prediction, save/restore, etc...'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 50999  | total loss: \u001b[1m\u001b[32m1.17452\u001b[0m\u001b[0m | time: 2.467s\n",
      "| Adam | epoch: 1000 | loss: 1.17452 - acc: 0.4474 -- iter: 400/401\n",
      "Training Step: 51000  | total loss: \u001b[1m\u001b[32m1.15262\u001b[0m\u001b[0m | time: 2.480s\n",
      "| Adam | epoch: 1000 | loss: 1.15262 - acc: 0.4526 -- iter: 401/401\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Kranthi Kumar\\chatBotMajorProject_YT\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load(\"model.tflearn\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True) #We will run it for 1000 epochs (the network \n",
    "                                                                #will see all data 1000 times) with a batch size of 8.'''\n",
    "    \n",
    "    model.save(\"model.tflearn\")  # acc: 0.5106 -- iter: 401/401 for 1000 epochs\n",
    "                                 # acc: 0.4652 -- iter: 401/401 for 1500 epochs acc: 0.5106 -- iter: 401/401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create function to give input to the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [lemmatizer.lemmatize(word.lower()) for word in s_words]\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GUI Chat window and other necessary components with tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GUI with tkinter\n",
    "import tkinter\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to remove special chars and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special(m):\n",
    "    m1=m.replace(\",\",\" \")\n",
    "    nestr = re.sub(r'[^a-zA-Z0-9 ]',r'',m1)\n",
    "    return nestr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(msg):\n",
    "    results = model.predict([bag_of_words(remove_special(msg), words)])[0]\n",
    "    results_index = numpy.argmax(results)\n",
    "    tag = labels[results_index]\n",
    "    if(results[results_index]>0.7):\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                t=tg['tag']\n",
    "                responses = tg['responses']\n",
    "                syms=tg[\"patterns\"]\n",
    "                info=\",\"\n",
    "                info=info.join(syms[1:])\n",
    "        \n",
    "        if t not in [\"greetings\",\"name\",\"goodbye\",\"thanks\",\"options\",\"diseases\"]:\n",
    "            return random.choice(responses),info\n",
    "        else:\n",
    "            return random.choice(responses),\"\"\n",
    "    else:\n",
    "        return \"Sorry,I'm unable to figure out.Please add some more symptoms.\",\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    s=msg\n",
    "    s=s.replace(\" \",\"\")\n",
    "    \n",
    "    ChatLog.config(state=NORMAL)\n",
    "    ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "    ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "    if s==\"\":\n",
    "        ChatLog.insert(END, \"DocBot: \" + \"You entered nothing.Please enter your symptoms.\" + '\\n\\n')\n",
    "    \n",
    "    elif s.isnumeric():\n",
    "        ChatLog.insert(END, \"DocBot: \" + \"You entered number.Please enter your symptoms.\" + '\\n\\n')\n",
    "    \n",
    "    elif s in \".,!@#$%^&*()_+={[}];:<>?\":\n",
    "        ChatLog.insert(END, \"DocBot: \" + \"You entered a special symbol.Please enter your symptoms.\" + '\\n\\n')\n",
    " \n",
    "    else:\n",
    "        res,add_info=chatbot_response(msg)\n",
    "\n",
    "        if res==\"Sorry,I'm unable to figure out.Please add some more symptoms.\":\n",
    "            if s.isalnum():\n",
    "                flag=0\n",
    "                for i in symptoms:\n",
    "                    if(msg.lower()==i.lower()):\n",
    "                        ChatLog.insert(END, \"DocBot: \" + res + '\\n\\n')\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0:\n",
    "                    ChatLog.insert(END, \"DocBot: \" + \"You entered alphanumeric.Try again with the symptoms I have known.\" + '\\n\\n')\n",
    "            else:\n",
    "                ChatLog.insert(END, \"DocBot: \" + \"Sorry,I can't understand you.Try again entering your symptoms.\" + '\\n\\n')\n",
    "\n",
    "                \n",
    "        else:\n",
    "            ChatLog.insert(END, \"DocBot: \" + res + '\\n\\n')\n",
    "            if add_info!=\"\":\n",
    "                ChatLog.insert(END, \"DocBot: It's symptoms include : \" + add_info + '\\n\\n')\n",
    "\n",
    "    ChatLog.config(state=DISABLED)\n",
    "    ChatLog.yview(END)\n",
    "    \n",
    "    if msg.lower()==\"quit\":\n",
    "        base.destroy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Tk()\n",
    "base.title(\"DocBot\")\n",
    "base.geometry(\"400x500\")\n",
    "base.resizable(width=TRUE, height=TRUE)\n",
    "\n",
    "#Create Chat window\n",
    "ChatLog = Text(base, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
    "\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "#Bind scrollbar to Chat window\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "#Create Button to send message\n",
    "SendButton = Button(base, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                    bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                    command= send )\n",
    "\n",
    "#Create the box to enter message\n",
    "EntryBox = Text(base, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
    "#EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "\n",
    "#Place all components on the screen\n",
    "scrollbar.place(x=376,y=6, height=386)\n",
    "ChatLog.place(x=6,y=6, height=386, width=370)\n",
    "EntryBox.place(x=128, y=401, height=90, width=265)\n",
    "SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '(', ')', ',', 'a', 'abdom', 'abdomin', 'abnorm', 'about', 'acid', 'acn', 'acut', 'alcohol', 'allergy', 'alt', 'an', 'and', 'anxy', 'anyon', 'appetit', 'around', 'arthrit', 'asthm', 'attack', 'awesom', 'b', 'back', 'bal', 'be', 'behind', 'bel', 'benign', 'blackhead', 'blad', 'blee', 'blist', 'blood', 'bloody', 'blur', 'body', 'bowel', 'bp', 'brain', 'breath', 'breathless', 'brittl', 'bronch', 'bru', 'buddy', 'burn', 'bye', 'c', 'calf', 'can', 'cas', 'cerv', 'chat', 'chest', 'chick', 'chil', 'cholestas', 'chronic', 'cold', 'com', 'common', 'congest', 'constip', 'consum', 'cont', 'continu', 'cough', 'could', 'covid-19', 'cramp', 'crust', 'd', 'dark', 'dehydr', 'dengu', 'dent', 'depress', 'diabet', 'diarrhoe', 'difficul', 'dimorph', 'dischrom', 'discomfort', 'diseas', 'dist', 'distort', 'disturb', 'dizzy', 'do', 'drug', 'dry', 'dur', 'dust', 'e', 'enlarg', 'erupt', 'excess', 'excessive_hunger', 'extrem', 'ey', 'fac', 'fail', 'famy', 'fast', 'fath', 'fatigu', 'feel', 'fev', 'fil', 'fluid', 'for', 'foul', 'from', 'fung', 'gain', 'gas', 'gastroenterit', 'gerd', 'goodby', 'headach', 'heart', 'hello', 'help', 'hemmorhoid', 'hemorrh', 'hepatit', 'hey', 'hi', 'high', 'hip', 'hist', 'hol', 'how', 'hung', 'hypertend', 'hyperthyroid', 'hypoglycem', 'hypothyroid', 'impetigo', 'in', 'increased_appetite', 'indigest', 'infect', 'inflam', 'inject', 'intern', 'irregul', 'irrit', 'is', 'itch', 'jaund', 'joint', 'kne', 'know', 'lack', 'lat', 'leg', 'lethargy', 'level', 'lik', 'limb', 'lip', 'liv', 'look', 'loss', 'lymph', 'mala', 'malar', 'me', 'menstru', 'micturit', 'migrain', 'mild', 'mood', 'mov', 'mucoid', 'musc', 'nail', 'nam', 'nause', 'neck', 'next', 'nic', 'nod', 'nos', 'obes', 'of', 'off', 'on', 'ooz', 'osteoarthr', 'ov', 'overload', 'pain', 'palpit', 'paralys', 'paroxyms', 'pass', 'patch', 'peel', 'pept', 'phlegm', 'pil', 'pimpl', 'pneumon', 'polyur', 'posit', 'pox', 'press', 'promin', 'provid', 'psorias', 'puffy', 'pus', 'rash', 'rat', 'react', 'receiv', 'red', 'reg', 'restless', 'runny', 'rusty', 'scur', 'see', 'sensor', 'sev', 'shiv', 'sid', 'silv', 'sin', 'skin', 'slur', 'smal', 'smel', 'sneez', 'sor', 'speech', 'spin', 'spondylos', 'spot', 'sput', 'stiff', 'stomach', 'stool', 'sug', 'sunk', 'support', 'swe', 'swel', 'swing', 'swol', 'team', 'temp', 'thank', 'that', 'the', 'ther', 'throat', 'thyroid', 'til', 'tim', 'tingl', 'to', 'tongu', 'tox', 'tract', 'train', 'transfus', 'tuberculos', 'typho', 'typhoid', 'ulc', 'unsteady', 'unsteril', 'up', 'urin', 'varicos', 'vein', 'vertigo', 'vessel', 'vis', 'vomit', 'walk', 'wat', 'weak', 'weight', 'wer', 'what', 'which', 'who', 'whoop', 'yellow', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "#LancasterStemmer\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LancasterStemmer\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '(', ')', ',', 'a', 'abdomen', 'abdominal', 'abnormal', 'about', 'acidity', 'acne', 'acute', 'alcohol', 'alcoholic', 'allergy', 'altered', 'anal', 'and', 'anus', 'anxiety', 'anyone', 'appetite', 'around', 'arthritis', 'asthma', 'attack', 'awesome', 'b', 'back', 'balance', 'be', 'behind', 'belly', 'benign', 'blackhead', 'bladder', 'bleeding', 'blister', 'blood', 'bloody', 'blurred', 'body', 'bowel', 'bp', 'brain', 'breathing', 'breathlessness', 'brittle', 'bronchial', 'bruising', 'buddy', 'burning', 'bye', 'c', 'calf', 'can', 'case', 'cervical', 'chatting', 'chest', 'chicken', 'chill', 'cholestasis', 'chronic', 'cold', 'coma', 'common', 'concentration', 'congestion', 'constipation', 'consumption', 'continuous', 'cough', 'could', 'covid-19', 'cramp', 'crust', 'd', 'dark', 'dehydration', 'dengue', 'dent', 'depression', 'diabetes', 'diarrhoea', 'difficulty', 'dimorphic', 'dischromic', 'discomfort', 'disease', 'distention', 'distorted', 'disturbance', 'dizziness', 'do', 'drug', 'drying', 'during', 'dusting', 'e', 'enlarged', 'eruption', 'excessive', 'excessive_hunger', 'extremeties', 'eye', 'face', 'failure', 'family', 'fast', 'father', 'fatigue', 'feel', 'fever', 'filled', 'fluid', 'for', 'foul', 'from', 'fungal', 'gain', 'gas', 'gastroenteritis', 'gerd', 'goodbye', 'headache', 'heart', 'hello', 'help', 'helpful', 'helping', 'hemmorhoids', 'hemorrhage', 'hepatitis', 'hey', 'hi', 'high', 'hip', 'history', 'hola', 'how', 'hunger', 'hypertension', 'hyperthyroidism', 'hypoglycemia', 'hypothyroidism', 'impetigo', 'in', 'increased_appetite', 'indigestion', 'infection', 'inflammatory', 'injection', 'internal', 'irregular', 'irritability', 'irritation', 'is', 'itching', 'jaundice', 'joint', 'knee', 'know', 'lack', 'later', 'leg', 'lethargy', 'level', 'like', 'limb', 'lip', 'liver', 'look', 'loss', 'lymph', 'malaise', 'malaria', 'me', 'menstruation', 'micturition', 'migraine', 'mild', 'mood', 'movement', 'mucoid', 'muscle', 'nail', 'name', 'nausea', 'neck', 'next', 'nice', 'nodal', 'node', 'nose', 'obesity', 'of', 'offered', 'on', 'one', 'ooze', 'osteoarthristis', 'over', 'overload', 'pain', 'painful', 'palpitation', 'paralysis', 'paroxymsal', 'passage', 'patch', 'peeling', 'peptic', 'phlegm', 'pile', 'pimple', 'pneumonia', 'polyuria', 'positional', 'pox', 'pressure', 'prominent', 'provide', 'psoriasis', 'pu', 'puffy', 'rash', 'rate', 'reaction', 'receiving', 'red', 'redness', 'region', 'restlessness', 'runny', 'rusty', 'scurring', 'see', 'sensorium', 'severe', 'shivering', 'side', 'silver', 'sinus', 'skin', 'slurred', 'small', 'smell', 'sneezing', 'sore', 'speech', 'spinning', 'spondylosis', 'spot', 'spotting', 'sputum', 'stiff', 'stiffness', 'stomach', 'stool', 'sugar', 'sunken', 'support', 'sweating', 'swelled', 'swelling', 'swing', 'swollen', 'team', 'temperature', 'thank', 'thanks', 'thats', 'the', 'there', 'throat', 'thyroid', 'till', 'time', 'tingling', 'to', 'tongue', 'toxic', 'tract', 'trained', 'transfusion', 'tuberculosis', 'typhoid', 'typhos', 'ulcer', 'unsteadiness', 'unsterile', 'up', 'urinary', 'urination', 'urine', 'varicose', 'vein', 'vertigo', 'vessel', 'vision', 'visual', 'vomiting', 'walking', 'watering', 'weakness', 'weight', 'were', 'what', 'whats', 'which', 'who', 'whooping', 'yellow', 'yellowing', 'yellowish', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "#WordNetLemmatizer\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WordNetLemmatizer\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Authenticated OK\n"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "anvil.server.connect(\"NTBDZOFMUYGOPQV2N4B5ZIV2-XCKWEJIILBT3FLCU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "\n",
    "\n",
    "def DocBot(msg):\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import re\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def remove_special(m):\n",
    "        m1=m.replace(\",\",\" \")\n",
    "        nestr = re.sub(r'[^a-zA-Z0-9 ]',r'',m1)\n",
    "        return nestr\n",
    "    \n",
    "    def bag_of_words(s, words):\n",
    "        bag = [0 for _ in range(len(words))]\n",
    "        s_words = nltk.word_tokenize(s)\n",
    "        s_words = [lemmatizer.lemmatize(word.lower()) for word in s_words]\n",
    "        for se in s_words:\n",
    "            for i, w in enumerate(words):\n",
    "                if w == se:\n",
    "                    bag[i] = 1\n",
    "        return numpy.array(bag)\n",
    "                \n",
    "    results = model.predict([bag_of_words(remove_special(msg), words)])[0]\n",
    "    results_index = numpy.argmax(results)\n",
    "    tag = labels[results_index]\n",
    "    if(results[results_index]>0.7):\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                t=tg['tag']\n",
    "                responses = tg['responses']\n",
    "                syms=tg[\"patterns\"]\n",
    "                info=\",\"\n",
    "                info=info.join(syms[1:])\n",
    "        if t not in [\"greetings\",\"name\",\"goodbye\",\"thanks\",\"options\",\"diseases\"]:\n",
    "            return random.choice(responses),info\n",
    "        else:\n",
    "            return random.choice(responses),\"\"\n",
    "    else:\n",
    "        return \"Sorry,I'm unable to figure out.Please add some more symptoms.\",\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://docbot.anvil.app'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anvil.server.get_app_origin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mp] *",
   "language": "python",
   "name": "conda-env-mp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
