{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "#stemmer = LancasterStemmer()\n",
    "#nltk.download('punkt')  Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences\n",
    "#by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.\n",
    "#nltk.download('wordnet') WordNet is a lexical database for the English language, which was created by Princeton,and is\n",
    "#part of the NLTK corpus. You can use WordNet alongside the NLTK module to find the meanings of words, synonyms and more.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"diseases.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [lemmatizer.lemmatize(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms=[\"hi\",\"hello\",\"bye\",\"thanks\",\"whats up\",\"Hey\"]\n",
    "with open(\"diseases1.csv\") as file1:\n",
    "    symp = csv.reader(file1, delimiter=',')\n",
    "    for row in symp:\n",
    "        for s in row:\n",
    "            symptoms.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\layers\\core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensorflow.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
    "\n",
    "#We are building a 3-layers neural network using TFLearn. We need to specify the shape of our input data. In our case,\n",
    "#each sample has a total of 318 features and we will process samples per batch to save memory, so our data \n",
    "#input shape is [None, 318] ('None' stands for an unknown dimension, so we can change the total number of samples \n",
    "#that are processed in a batch).\n",
    "net = tflearn.input_data(shape=[None, len(training[0])]) #to know length of features we use len(training[0])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") #Softmax function is used in the output layer of a \n",
    "                                    #neural net to represent a probability distribution of possible outcomes of the network.\n",
    "net = tflearn.regression(net) #used in TFLearn to apply a regression (linear or logistic) to the provided input. It requires\n",
    "                              #to specify a TensorFlow gradient descent optimizer 'optimizer' that will minimize the provided \n",
    "                              #loss function 'loss'\n",
    "\n",
    "model = tflearn.DNN(net) #TFLearn provides a model wrapper 'DNN' that can automatically performs a neural network\n",
    "                         #classifier tasks, such as training, prediction, save/restore, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kranthi Kumar\\Anaconda3\\envs\\mp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Kranthi Kumar\\chatBotMajorProject_YT\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load(\"model.tflearn\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True) #We will run it for 1000 epochs (the network \n",
    "                                                                #will see all data 1000 times) with a batch size of 8.\n",
    "    \n",
    "    model.save(\"model.tflearn\")  # acc: 0.5106 -- iter: 401/401 for 1000 epochs\n",
    "                                 # acc: 0.4652 -- iter: 401/401 for 1500 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create function to give input to the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [lemmatizer.lemmatize(word.lower()) for word in s_words]\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GUI Chat window and other necessary components with tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating GUI with tkinter\n",
    "import tkinter\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to remove special chars and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special(m):\n",
    "    m1=m.replace(\",\",\" \")\n",
    "    nestr = re.sub(r'[^a-zA-Z0-9 ]',r'',m1)\n",
    "    return nestr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(msg):\n",
    "    results = model.predict([bag_of_words(remove_special(msg), words)])[0]\n",
    "    results_index = numpy.argmax(results)\n",
    "    tag = labels[results_index]\n",
    "    if(results[results_index]>0.7):\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                t=tg['tag']\n",
    "                responses = tg['responses']\n",
    "                syms=tg[\"patterns\"]\n",
    "                info=\",\".join(syms[1:])\n",
    "        \n",
    "        if t not in [\"greetings\",\"name\",\"goodbye\",\"thanks\",\"options\",\"diseases\"]:\n",
    "            return random.choice(responses),info\n",
    "        else:\n",
    "            return random.choice(responses),\"\"\n",
    "    else:\n",
    "        return \"Sorry,I'm unable to figure out.Please add some more symptoms.\",\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "dis=[\"brain hemorrhage\",\"piles\",\"BP\",\"Acne\",\"Alcoholic hepatitis\",\"Allergy\",\"Arthritis\",\"Benign Paroxymsal Positional Vertigo\",\"Bronchial Asthma\",\"COVID-19\",\"Cervical spondylosis\",\"Chicken pox\",\"Chronic cholestasis\",\"Common Cold\",\"Dengue,Diabetes\",\"Dimorphic hemmorhoids(piles)\",\"Drug Reaction\",\"Fungal infection\",\"GERD\",\"Gastroenteritis\",\"Heart attack\",\"Hepatitis A\",\"Hepatitis B\",\"Hepatitis C\",\"Hepatitis D\",\"Hepatitis E\",\"Hypertension\",\"Hyperthyroidism\",\"Hypoglycemia\",\"Hypothyroidism\",\"Impetigo\",\"Jaundice\",\"Malaria\",\"Migraine\",\"Osteoarthristis\",\"Paralysis (brain hemorrhage)\",\"Peptic ulcer disease\",\"Pneumonia\",\"Psoriasis\",\"Tuberculosis\",\"Typhoid\",\"Urinary tract infection\",\"Varicose veins\",\"fever\"]\n",
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    s=msg\n",
    "    s=s.replace(\" \",\"\")\n",
    "    \n",
    "    ChatLog.config(state=NORMAL)\n",
    "    ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "    ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
    "    if s==\"\":\n",
    "        ChatLog.insert(END, \"DocBot: \" + \"You entered nothing.Please enter your symptoms.\" + '\\n\\n')\n",
    "    \n",
    "    elif s.isnumeric():\n",
    "        ChatLog.insert(END, \"DocBot: \" + \"You entered number.Please enter your symptoms.\" + '\\n\\n')\n",
    "    \n",
    "    elif s in \".,!@#$%^&*()_+={[}];:<>?\":\n",
    "        ChatLog.insert(END, \"DocBot: \" + \"You entered a special symbol.Please enter your symptoms.\" + '\\n\\n')\n",
    " \n",
    "    else:\n",
    "        res,add_info=chatbot_response(msg)\n",
    "\n",
    "        if res==\"Sorry,I'm unable to figure out.Please add some more symptoms.\":\n",
    "            if s.isalnum():\n",
    "                flag=0\n",
    "                for i in symptoms:\n",
    "                    if(msg.lower()==i.lower()):\n",
    "                        l.append(msg)\n",
    "                        r=\",\".join(l)\n",
    "                        res,add_info=chatbot_response(r)\n",
    "                        if res==\"Sorry,I'm unable to figure out.Please add some more symptoms.\":\n",
    "                            ChatLog.insert(END, \"DocBot: \" + res + '\\n\\n')\n",
    "                        else:\n",
    "                            ChatLog.insert(END, \"DocBot: \" + res + '\\n\\n')\n",
    "                            entered_symptoms=\",\".join(l)\n",
    "                            #ChatLog.insert(END, \"DocBot: Symptoms you entered : \" + entered_symptoms + '\\n\\n')\n",
    "                            ChatLog.insert(END, \"DocBot: It's symptoms include : \" + add_info + '\\n\\n')\n",
    "                            percentage=len(list(entered_symptoms.split(\",\")))*100/len(list(add_info.split(\",\")))\n",
    "                            ChatLog.insert(END, \"DocBot: I am \" + str(percentage) + \"% confident that you may have this disease.\" '\\n\\n')       \n",
    "                            l.clear()\n",
    "                        flag=1\n",
    "                        break\n",
    "                if flag==0:\n",
    "                    ChatLog.insert(END, \"DocBot: \" + \"You entered alphanumeric.Try again with the symptoms I have known.\" + '\\n\\n')\n",
    "            else:\n",
    "                ChatLog.insert(END, \"DocBot: \" + \"Sorry,I can't understand you.Try again entering your symptoms.\" + '\\n\\n')\n",
    "\n",
    "                \n",
    "        else:\n",
    "            ChatLog.insert(END, \"DocBot: \" + res + '\\n\\n')\n",
    "            if add_info!=\"\":\n",
    "                f=0\n",
    "                for i in dis:\n",
    "                    if(msg.lower()==i.lower()):\n",
    "                        ChatLog.insert(END, \"DocBot: It's symptoms include : \" + add_info + '\\n\\n')\n",
    "                        f=1\n",
    "                        break\n",
    "                if(f==0):\n",
    "                    l.append(msg)\n",
    "                    entered_symptoms=\",\".join(l)\n",
    "                    #ChatLog.insert(END, \"DocBot: Symptoms you entered : \" + entered_symptoms + '\\n\\n')\n",
    "                    ChatLog.insert(END, \"DocBot: It's symptoms include : \" + add_info + '\\n\\n')\n",
    "                    percentage=len(list(entered_symptoms.split(\",\")))*100/len(list(add_info.split(\",\")))\n",
    "                    ChatLog.insert(END, \"DocBot: I am \" + str(percentage) + \"% confident that you may have this disease.\" '\\n\\n')\n",
    "            l.clear()\n",
    "\n",
    "    ChatLog.config(state=DISABLED)\n",
    "    ChatLog.yview(END)\n",
    "    \n",
    "    if msg.lower()==\"quit\":\n",
    "        l.clear()\n",
    "        base.destroy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Tk()\n",
    "base.title(\"DocBot\")\n",
    "base.geometry(\"400x500\")\n",
    "base.resizable(width=TRUE, height=TRUE)\n",
    "\n",
    "#Create Chat window\n",
    "ChatLog = Text(base, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
    "\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "#Bind scrollbar to Chat window\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "\n",
    "#Create Button to send message\n",
    "SendButton = Button(base, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                    bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                    command= send )\n",
    "\n",
    "#Create the box to enter message\n",
    "EntryBox = Text(base, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
    "#EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "\n",
    "#Place all components on the screen\n",
    "scrollbar.place(x=376,y=6, height=386)\n",
    "ChatLog.place(x=6,y=6, height=386, width=370)\n",
    "EntryBox.place(x=128, y=401, height=90, width=265)\n",
    "SendButton.place(x=6, y=401, height=90)\n",
    "\n",
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Authenticated OK\n"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "anvil.server.connect(\"N3VOJ64VKF5RPRCK5FSL44OG-XCKWEJIILBT3FLCU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "\n",
    "def DocBot(msg):\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import re\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def remove_special(m):\n",
    "        m1=m.replace(\",\",\" \")\n",
    "        nestr = re.sub(r'[^a-zA-Z0-9 ]',r'',m1)\n",
    "        return nestr\n",
    "    \n",
    "    def bag_of_words(s, words):\n",
    "        bag = [0 for _ in range(len(words))]\n",
    "        s_words = nltk.word_tokenize(s)\n",
    "        s_words = [lemmatizer.lemmatize(word.lower()) for word in s_words]\n",
    "        for se in s_words:\n",
    "            for i, w in enumerate(words):\n",
    "                if w == se:\n",
    "                    bag[i] = 1\n",
    "        return numpy.array(bag)\n",
    "                \n",
    "    results = model.predict([bag_of_words(remove_special(msg), words)])[0]\n",
    "    results_index = numpy.argmax(results)\n",
    "    tag = labels[results_index]\n",
    "    if(results[results_index]>0.7):\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                t=tg['tag']\n",
    "                responses = tg['responses']\n",
    "                syms=tg[\"patterns\"]\n",
    "                info=\",\"\n",
    "                info=info.join(syms[1:])\n",
    "        if t not in [\"greetings\",\"name\",\"goodbye\",\"thanks\",\"options\",\"diseases\"]:\n",
    "            return random.choice(responses),info\n",
    "        else:\n",
    "            return random.choice(responses),\"\"\n",
    "    else:\n",
    "        return \"Sorry,I'm unable to figure out.Please add some more symptoms.\",\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://docbot.anvil.app'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anvil.server.get_app_origin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mp] *",
   "language": "python",
   "name": "conda-env-mp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
